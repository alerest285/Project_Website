<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8"/>
        <title>Robotic Arm</title>
    </head>
<body>
	
	
	
	
	<div style="text-align:center">
	<h1>
	<b>3DOF Robotic arm with inverse kinematics</b>
	</h1>
	<h3>
	<i>Alejo Restrepo</i>
	</h3>
	<h4>2021</h4>
        </div>
	
	
	
	
	<table align=center width=300px>
	 <tr>
	  <td align=center width=50px>
	   <center>
	    <span style="font-size:24px">
	     <a href='https://www.youtube.com/watch?v=BGVK2FOS8_U'>[Video]</a>
	    </span>
	   </center>
	  </td>
	  <td align=center width=50px>
	   <center>
	    <span style="font-size:24px">
		<a href='https://github.com/alerest285/3DOF-Robotic-Arm'>[GitHub]</a>
	    </span>
	   </center>
	  </td>
        </table>
   
		
		
	<table align=center width=300px>
	 <tr>
	  <td align=center width=50px>
	   <center>
	    <span style="font-size:24px">
	     <img src="images.jpeg" alt="3DOF Robot Arm">
	    </span>
	   </center>
	  </td> 
	  <td align=center width=50px>
	   <center>
	    <span style="font-size:24px">
		<video width="300" height="200" controls>
                     <source src="3 DOF articulated robot arm with Cartesian coordinate control.mp4" type="video/mp4" type="video/mp4"/>
                </video>
	    </span>
	   </center>
	  </td>
        </table>
	
	
	
	
	
	
	
	
	<div style="width:800px; margin:0 auto; text-align:justify;">
            Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video.
            Our framework addresses this problem without using any annotation or prior information about the specific object to animate.  Once trained on a set of videos depicting
            objects of the same category (<i>e.g.</i> faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion
            information using a self-supervised formulation.  To support complex motions, we use a representation consisting of a set of learned keypoints along with their
            local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and
            the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories
       </div>
        
		
	

			
</body>
</html>
